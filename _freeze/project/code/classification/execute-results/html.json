{
  "hash": "8935457cab0ae9a14935adb100b3166e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Code Display\"\nexecute:\n  eval: false  # Disable execution for this file only\n---\n\n::: {.cell}\n\n```{.r .cell-code}\n# load all required packages \nlibrary(ROSE)\nlibrary(ISLR)\nlibrary(caret)\nlibrary(kernlab)\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(tune)\nlibrary(dials)\nlibrary(ranger)\nlibrary(workflows)\nlibrary(rsample)\nlibrary(yardstick)\nlibrary(kknn)\nlibrary(lattice)\nlibrary(stacks)\ntidymodels_prefer()\n\n# load the original training data \ntrain <- read_csv('train_class.csv',show_col_types = FALSE)\n\n# delete the id, name and total_population to remove non-predictors and ovoid overfitting.  \ntrain <- train %>% \n  select(-id, -name, -x0001e)\n\n#create recipe from training data \nvote_recipe1 <- recipe(winner ~ . , data = train) %>%\n  step_impute_mean(all_numeric_predictors(), -all_outcomes()) %>%\n  step_normalize(all_numeric_predictors()) %>%\n  step_dummy(all_nominal(), -all_outcomes())\n\n# set engine and mode for rain forest \nrf_initial_model <- rand_forest(trees = 100) %>%\n  set_engine('ranger', importance = 'impurity') %>%\n  set_mode('classification')\n\n# Workflow to fit initial model\ninitial_rf_wkfl <- workflow() %>%\n  add_model(rf_initial_model) %>%\n  add_recipe(vote_recipe1)\n\n# Fit model to get feature importance\nset.seed(100)\ninitial_fit <- fit(initial_rf_wkfl, data = train)\n\n# Extracting feature importance\nimportance_df <- vip::vi(initial_fit, method = \"model\")\n# Check the structure of importance_df\nprint(str(importance_df))\n\n#select the top 20 important predictors\ntop_features <- importance_df$Variable[1:20]\n\n# select only the response variable and top 20 predictors \ntrain <- train %>%\n  select(winner, all_of(top_features))\n\n#load the test data \ntest <- read_csv('test_class.csv')\n\n# select only the id column and the top 20 predictors from training \ntest <- test %>%\n  select(id, all_of(top_features))\n\n# set up cvfolds with v = 10\nset.seed(100)\nfolds <- vfold_cv(train,v=10)\n\n\n## model1 \n\n# Update recipe to include only the top features\nvote_recipe <- recipe(winner ~ . , data = train) %>%\n  step_impute_mean(all_numeric_predictors(), -all_outcomes()) %>%\n  step_normalize(all_numeric_predictors()) %>%\n  step_dummy(all_nominal(), -all_outcomes())\n\n#create the new model with ranger and classification \nrf_model1 <- rand_forest() %>%\n  set_engine('ranger')%>%\n  set_mode('classification')\n\n\n# Update the workflow with the refined recipe\nrf_wkfl1 <- workflow() %>%\n  add_model(rf_model1)%>%\n  add_recipe(vote_recipe)\n\n#Convert \n\n#update random forest model with mtry, min_m and trees \nrf_model1 <- rand_forest(\n  mtry = tune(),     \n  min_n = tune(),    \n  trees = 1000      \n) %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"classification\")\n\n#update workflow with tuning model\nrf_tune_wkfl1 <- rf_wkfl1 %>%\n  update_model(rf_model1)\n\n#set up tuning parameters \nrf_params1 <- parameters(\n  mtry(range = c(1, ncol(train) - 1)),\n  min_n(range = c(1, 10))\n)\n\n# Set up Bayesian Optimization\nbayes_opt1 <- tune_bayes(\n  rf_model1,\n  vote_recipe,\n  resamples = vfold_cv(train, v = 5),\n  param_info = rf_params1,\n  initial = 10,  \n  iter = 20,     \n  metrics = metric_set(roc_auc, accuracy)\n)\n#check the model results \nbayes_opt1 %>%\n  collect_metrics()\n\n#select the best model based on their roc_auc results \nbest_params1 <- select_best(bayes_opt1, metric = \"roc_auc\")\n\n#create final workflow with the best model\nfinal_rf_wkfl1 <- rf_tune_wkfl1 %>%\n  finalize_workflow(best_params1)\nfinal_rf_wkfl1\n\n#resample the model with the newest workflow and folds\nset.seed(100)\nfinal_rf_res1 <- fit_resamples(\n  final_rf_wkfl1,\n  resamples =folds,\n  metrics = metric_set(accuracy,roc_auc),control=control_resamples(save_workflow = TRUE)\n)\n\ncollect_metrics(final_rf_res1)\n\n# fit the training data to the workflow \nfinal_rf_fit1 <-final_rf_wkfl1 %>%\n  fit(data=train)\n\n#make predictions based on the model \nrf_predictions1 <- final_rf_fit1 %>%\n  predict(new_data=test)\n\n#create results table \nresults_rf1 <- test %>%\n  select(id) %>%\n  bind_cols(rf_predictions1)%>%\n  rename(id = id, winner = .pred_class)\n\nhead(results_rf1,15)\n\n#output results \nwrite_csv(results_rf1,'rf_bayes_class_original_top20_final.csv')\n\nrm(list = ls())\n\n\n## model2 stacking\n\n# load all required packages \nlibrary(ROSE)\nlibrary(ISLR)\nlibrary(caret)\nlibrary(kernlab)\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(tune)\nlibrary(dials)\nlibrary(ranger)\nlibrary(workflows)\nlibrary(rsample)\nlibrary(yardstick)\nlibrary(kknn)\nlibrary(lattice)\nlibrary(stacks)\ntidymodels_prefer()\n\n# load the original training data \ntrain <- read_csv('train_class.csv',show_col_types = FALSE)\n\n# delete the id, name and total_population to remove non-predictors and ovoid overfitting.  \ntrain <- train %>% \n  select(-id, -name, -x0001e)\n\n#create recipe from training data \nvote_recipe1 <- recipe(winner ~ . , data = train) %>%\n  step_impute_mean(all_numeric_predictors(), -all_outcomes()) %>%\n  step_normalize(all_numeric_predictors()) %>%\n  step_dummy(all_nominal(), -all_outcomes())\n\n# set engine and mode for rain forest \nrf_initial_model <- rand_forest(trees = 100) %>%\n  set_engine('ranger', importance = 'impurity') %>%\n  set_mode('classification')\n\n# Workflow to fit initial model\ninitial_rf_wkfl <- workflow() %>%\n  add_model(rf_initial_model) %>%\n  add_recipe(vote_recipe1)\n\n# Fit model to get feature importance\nset.seed(100)\ninitial_fit <- fit(initial_rf_wkfl, data = train)\n\n# Extracting feature importance\nimportance_df <- vip::vi(initial_fit, method = \"model\")\n# Check the structure of importance_df\nprint(str(importance_df))\n\n#select the top 20 important predictors\ntop_features <- importance_df$Variable[1:20]\n\n# select only the response variable and top 20 predictors \ntrain <- train %>%\n  select(winner, all_of(top_features))\n\n#load the test data \ntest <- read_csv('test_class.csv')\n\n# select only the id column and the top 20 predictors from training \ntest <- test %>%\n  select(id, all_of(top_features))\n\n# set up cvfolds with v = 10\nset.seed(100)\nfolds <- vfold_cv(train,v=10, repeats = 3)\n\n# Set control for tuning\nctrl_grid <- control_stack_grid()\n\n# Set up resampling\nset.seed(100)\ncv_folds <- vfold_cv(train, v = 10, repeats = 3)\n\n# Update recipe to include only the top features\nvote_recipe <- recipe(winner ~ . , data = train) %>%\n  step_impute_mean(all_numeric_predictors(), -all_outcomes()) %>%\n  step_normalize(all_numeric_predictors()) %>%\n  step_dummy(all_nominal(), -all_outcomes())\n\n# create random forest model \nrf_model <- rand_forest() %>%\n  set_engine('ranger')%>%\n  set_mode('classification')\n\n\n# Update the workflow with the refined recipe\nrf_wkfl <- workflow() %>%\n  add_model(rf_model)%>%\n  add_recipe(vote_recipe)\n\n#update model with tuning parameters mtry, min_n, trees  \nrf_tune_model <- rand_forest(\n  mtry = tune(),     # Indicate tuning\n  min_n = tune(),    # Indicate tuning\n  trees = 1000       # Fixed number of trees\n) %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"classification\")\n\n#update workflow with tuning model \nrf_tune_wkfl <- rf_wkfl %>%\n  update_model(rf_tune_model)\n\n#set up tuning parameters \nrf_params <- parameters(\n  mtry(range = c(1, ncol(train) - 1)),\n  min_n(range = c(1, 10))\n)\n\n# Set up Bayesian Optimization\nbayes_opt <- tune_bayes(\n  rf_tune_model,\n  vote_recipe,\n  resamples = vfold_cv(train, v = 5),\n  param_info = rf_params,\n  initial = 10,  # Number of randomly selected points to evaluate before the Bayesian optimization starts\n  iter = 20,     # Number of iterations of Bayesian optimization\n  metrics = metric_set(roc_auc, accuracy)\n)\n\nbayes_opt %>%\n  collect_metrics()\n\n#select the best model based on roc_auc \nbest_params <- select_best(bayes_opt, metric = \"roc_auc\")\n\n#update workflow with the selected model \nfinal_rf_wkfl <- rf_tune_wkfl %>%\n  finalize_workflow(best_params)\nfinal_rf_wkfl\n\n# resample the final workflow with folds \nset.seed(100)\nfolds <- vfold_cv(train,v=10)\n\nfinal_rf_res <- fit_resamples(\n  final_rf_wkfl,\n  resamples =folds,\n  metrics = metric_set(accuracy,roc_auc),control=control_resamples(save_workflow = TRUE)\n)\n\n## random forest 2\n\n#set up second random forest model \nrf_model2 <- rand_forest() %>%\n  set_engine('ranger')%>%\n  set_mode('classification')\n\n# Update the workflow with the refined recipe\nrf_wkfl2 <- workflow() %>%\n  add_model(rf_model2)%>%\n  add_recipe(vote_recipe)\n\n#update the second model with tuning parameters mtry, trees, min_n\nrf_tune_model2 <- rand_forest(\n  mtry = tune(),\n  trees = tune(),\n  min_n = tune()\n) %>%\n  set_engine('ranger')%>%\n  set_mode('classification')\n\n#update the workflow with tuning model \nrf_tune_wkfl2 <- rf_wkfl2 %>%\n  update_model(rf_tune_model2)\n\n#set up tuning parameters \nrf_grid2 <- grid_latin_hypercube(\n  mtry(range = c(1, 5)),\n  trees(range = c(50, 200)),\n  min_n(range = c(1, 10)),\n  size = 20\n)\n\n\n#apply grid tuning \nrf_tuning2 <- tune_grid(\n  rf_tune_wkfl2,\n  resamples = folds,\n  grid = rf_grid2,\n  metrics = metric_set(accuracy, roc_auc)\n)\n\nrf_tuning2 %>%\n  collect_metrics()\n\n#select the best model with roc_auc results \nbest_rf_model2 <- rf_tuning2 %>%\n  select_best(metric = 'roc_auc')\n\n#update the final workflow with the best model \nfinal_rf_wkfl2 <- rf_tune_wkfl2 %>%\n  finalize_workflow(best_rf_model2)\n\n#resample the data with the final workflow and folds \nset.seed(100)\nfinal_rf_res2 <- fit_resamples(\n  final_rf_wkfl2,\n  resamples =folds,\n  metrics = metric_set(accuracy,roc_auc),control=control_resamples(save_workflow = TRUE)\n)\n\n\n## Model 3 SVM-rbf with bayes optimization\n\n# Define the SVM model with tuning parameters\nsvm_rbf_model <- svm_rbf() %>%\n  set_engine(\"kernlab\") %>%\n  set_mode('classification') %>%\n  set_args(cost = tune(), rbf_sigma = tune())\n\n# Define the recipe\nvote_recipe_svm <- recipe(winner ~ ., data = train) %>%\n  step_impute_mean(all_numeric(), -all_outcomes()) %>%\n  step_normalize(all_numeric()) %>%\n  step_nzv(all_predictors())\n\n# Define the workflow\nsvm_rbf_wkfl <- workflow() %>%\n  add_model(svm_rbf_model) %>%\n  add_recipe(vote_recipe_svm)\n\n# Define the parameter set\nsvm_param <- extract_parameter_set_dials(svm_rbf_wkfl) %>%\n  update(\n    rbf_sigma = rbf_sigma(range = c(-7, -1)),\n    cost = cost(range = c(-5, 2))\n  )\n\n# Define the initial grid using Latin hypercube\nsvm_grid <- grid_latin_hypercube(\n  svm_param,\n  size = 30\n)\n\n# Set seed for reproducibility\nset.seed(100)\n\n# Initial tuning with grid search\nsvm_initial <- tune_grid(\n  svm_rbf_wkfl,\n  resamples = folds,\n  grid = svm_grid,\n  metrics = metric_set(accuracy, roc_auc)\n)\n\n# Collect initial metrics\ncollect_metrics(svm_initial)\n\n# Bayesian optimization control settings\nctrl <- control_bayes(verbose = FALSE)\n\n# Bayesian optimization\nset.seed(100)\nsvm_bo <- tune_bayes(\n  svm_rbf_wkfl,\n  resamples = folds,\n  metrics = metric_set(accuracy, roc_auc),\n  initial = svm_initial,\n  param_info = svm_param,\n  iter = 25,\n  control = ctrl\n)\n\n# Display best results\nsvm_bo %>% show_best(metric = 'roc_auc', n = 5)\n\n# Select the best model from Bayesian optimization\nbest_bayes <- svm_bo %>% select_best(metric = \"roc_auc\")\n\n# Finalize the workflow with the best parameters\nfinal_svm_rbf_wkfl <- finalize_workflow(svm_rbf_wkfl, best_bayes)\n\n# Perform resampling with the final workflow\nset.seed(100)\n\nfinal_svm_rbf_res <- fit_resamples(\n  final_svm_rbf_wkfl,\n  resamples = folds,\n  metrics = metric_set(accuracy, roc_auc),\n  control=control_stack_resamples()\n)\n\n# Collect final resampling metrics\ncollect_metrics(final_svm_rbf_res)\n\n## logistic model \n\n# Define the logistic regression model with glmnet, suitable for tuning\nlogistic_tuned <- logistic_reg(penalty = tune(), mixture = tune()) %>% \n  set_engine(\"glmnet\") %>% \n  set_mode(\"classification\")\n\n\n# Combine the recipe and model into a workflow\nworkflow_tuned <- workflow() %>%\n  add_model(logistic_tuned) %>%\n  add_recipe(vote_recipe)\n\n# Define a grid of hyperparameters\npenalty_vals <- penalty(range = c(-6, -1), trans = log10_trans()) # Log transformation\nmixture_vals <- mixture()\n\n# Create a regular grid\ntuning_grid <- grid_regular(\n  penalty_vals,\n  mixture_vals,\n  levels = 10\n)\n\ntune_results <- tune_grid(\n  workflow_tuned,\n  resamples = folds,\n  grid = tuning_grid,\n  metrics = metric_set(roc_auc, accuracy)\n)\n\ntune_results %>% collect_metrics()\n\n# Select the best model based on ROC AUC\nbest_model3 <- select_best(tune_results, metric = \"roc_auc\")\n\n# Finalize the workflow with the best model\nfinal_workflow <- finalize_workflow(workflow_tuned, best_model3)\n\n# Fit the finalized workflow to the resampled training data\nset.seed(100)  # Ensure reproducibility\nfinal_results <- fit_resamples(\n  final_workflow,\n  resamples = cv_folds,\n  metrics = metric_set(roc_auc, accuracy), \n  control=control_resamples(save_workflow = TRUE)\n)\n\n\n# stacking \n\n#set up for model stacking \nstack_control <- control_resamples(\n  save_pred = TRUE,   \n  save_workflow = TRUE  \n)\n\nset.seed(100)\n# Random Forest Model with bayes tuning resample \nrf_res1 <- fit_resamples(\n  final_rf_wkfl,\n  resamples =folds,\n  metrics = metric_set(roc_auc),\n  control= stack_control\n)\n#random forest model with grid tuning resample \nrf_res2 <- fit_resamples(\n  final_rf_wkfl2,\n  resamples =folds,\n  metrics = metric_set(roc_auc),\n  control=stack_control\n)\n#svm with bayes tuning resample \nsvm_res <- fit_resamples(\n  final_svm_rbf_wkfl,\n  resamples = folds,\n  metrics = metric_set(roc_auc), \n  control=stack_control\n)\n#logistics model resample \nlog_res <- fit_resamples(\n  final_workflow,\n  resamples = folds,\n  metrics = metric_set(roc_auc), \n  control=stack_control\n)\n\n\n# Stack the models\nstack <- stacks() %>%\n  add_candidates(rf_res1) %>%\n  add_candidates(rf_res2) %>%\n  add_candidates(log_res) %>%\n  add_candidates(svm_res)\n\n# Blend predictions\nstack <- stack %>%\n  blend_predictions() %>%\n  fit_members()\n\n#check the model by predicting the training data \ntrain_predictions <- predict(stack, new_data = train)\nresults_df <- train %>%\n  bind_cols(train_predictions)\n\nresults_df$winner <- as.factor(results_df$winner)\nresults_df$.pred_class <- as.factor(results_df$.pred_class)\n\n# Calculate accuracy\naccuracy_result <- accuracy(data = results_df, truth = \"winner\", estimate = \".pred_class\")\n\n# Print accuracy\nprint(accuracy_result)\n\n# predict the test dataset \nnew_prediction <- predict(stack, new_data = test)\n\nresults_stack <- test %>%\n  select(id) %>% \n  bind_cols(new_prediction)\n\nhead(results_stack)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}