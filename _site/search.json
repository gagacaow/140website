[
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Projects",
    "section": "",
    "text": "Title\n\n\n\n\n\n\nData Analysis of Life Expectancy in 2007\n\n\n\n\nImmediate Effects of Alcoholic Drinks on Memory\n\n\n\n\nAmazon Order Survey\n\n\n\n\n2020 Presidential Election\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "project/project2.html",
    "href": "project/project2.html",
    "title": "Immediate Effects of Alcoholic Drinks on Memory",
    "section": "",
    "text": "This research investigates the immediate effects of different alcoholic drinks on memory performance, specifically focusing on three beverages: Beer Regular, Guinness, and Red Wine. Utilizing a Latin Square design, the study considers age and Body Mass Index (BMI) as blocking factors to mitigate variability. Ninety participants were randomly selected from three islands and divided into age groups ([21, 31), [31, 41), and [41, 51)) and BMI categories ([0, 19.5), [19.5, 22.5), [22.5, ∞)). Memory performance was measured before and after the consumption of 250 mL of the assigned alcoholic drink. Analysis of Variance (ANOVA) was used to assess the impact of the beverages on memory scores. The findings indicate no significant difference in memory performance across the different alcoholic drinks, suggesting that the type of alcoholic beverage consumed does not differentially affect immediate memory recall.\nThis is my team’s reports for the Analysis"
  },
  {
    "objectID": "project/project4.html",
    "href": "project/project4.html",
    "title": "2020 Presidential Election",
    "section": "",
    "text": "This report explains the process of constructing classification models for a train dataset through supervised learning. The dataset used in this study is retrieved from Kaggle, for which the data of the election winner of each county where demographic information is recorded. The demographic information is estimated by the U.S Census Bureau. The dataset contains four files: train_class.csv, test_class.csv, col_descriptions.txt, and sample_submission.csv. For the purpose of this specific study, we use the train_class.csv and test_class.csv and modify them accordingly in the process of building the final classification model.\nThe objective of this study is to build a model such that it will predict the winner of a county, based on various predictors. According to the col_description.txt, we select the predictor columns that include distinct age levels, racial composition, and education level with statistical significance to our prediction.\nThis is my team’s report for the Classification Model\nThis is the code for the stacking model"
  },
  {
    "objectID": "me.html",
    "href": "me.html",
    "title": "Yechen (Gaga) Cao",
    "section": "",
    "text": "With a strong foundation in statistics, I specialize in data analysis and programming, proficient in Python, RStudio, and Java. My technical expertise is complemented by experience in Microsoft Office, which I use for effective data-driven decision-making and project management. Alongside my technical skills, I’ve developed a deep understanding of human behavior and business strategy through courses in psychology and business, as well as hands-on experience in HR, Marketing and Sales, where I gained insights into talent management and organizational dynamics.\nPassionate about blending technology and creativity, I aspire to make a meaningful impact in the industry, using my skills to help bring ideas to life and create memorable experiences.\nThis is my detailed CV"
  },
  {
    "objectID": "me.html#education",
    "href": "me.html#education",
    "title": "Yechen (Gaga) Cao",
    "section": "Education",
    "text": "Education\nUniversity of California, Los Angeles\nB.A. Statistics and Data Science    |    March 2025"
  },
  {
    "objectID": "me.html#experience",
    "href": "me.html#experience",
    "title": "Yechen (Gaga) Cao",
    "section": "Experience",
    "text": "Experience\nArc’teryx   |  Product Guide   |   Nov 2024 - Present\nUCLA Student Affair   |   Student Marketing and Operation Coordinator   |   Oct 2023 - Present\nDowntown Dolls  |   Modelling agent & Event planning   |   Apr 2023 - June 2023\nBloomage BioTechnology Corporation Limited   |   Brand Marketing   |   Jun 2022 - Sept 2022\nMSC Consulting   |   Marketing   | Jan 2022 - Jun 2022"
  },
  {
    "objectID": "project/classification.html",
    "href": "project/classification.html",
    "title": "Code Display",
    "section": "",
    "text": "# load all required packages \nlibrary(ROSE)\nlibrary(ISLR)\nlibrary(caret)\nlibrary(kernlab)\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(tune)\nlibrary(dials)\nlibrary(ranger)\nlibrary(workflows)\nlibrary(rsample)\nlibrary(yardstick)\nlibrary(kknn)\nlibrary(lattice)\nlibrary(stacks)\ntidymodels_prefer()\n\n# load the original training data \ntrain &lt;- read_csv('train_class.csv',show_col_types = FALSE)\n\n# delete the id, name and total_population to remove non-predictors and ovoid overfitting.  \ntrain &lt;- train %&gt;% \n  select(-id, -name, -x0001e)\n\n#create recipe from training data \nvote_recipe1 &lt;- recipe(winner ~ . , data = train) %&gt;%\n  step_impute_mean(all_numeric_predictors(), -all_outcomes()) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes())\n\n# set engine and mode for rain forest \nrf_initial_model &lt;- rand_forest(trees = 100) %&gt;%\n  set_engine('ranger', importance = 'impurity') %&gt;%\n  set_mode('classification')\n\n# Workflow to fit initial model\ninitial_rf_wkfl &lt;- workflow() %&gt;%\n  add_model(rf_initial_model) %&gt;%\n  add_recipe(vote_recipe1)\n\n# Fit model to get feature importance\nset.seed(100)\ninitial_fit &lt;- fit(initial_rf_wkfl, data = train)\n\n# Extracting feature importance\nimportance_df &lt;- vip::vi(initial_fit, method = \"model\")\n# Check the structure of importance_df\nprint(str(importance_df))\n\n#select the top 20 important predictors\ntop_features &lt;- importance_df$Variable[1:20]\n\n# select only the response variable and top 20 predictors \ntrain &lt;- train %&gt;%\n  select(winner, all_of(top_features))\n\n#load the test data \ntest &lt;- read_csv('test_class.csv')\n\n# select only the id column and the top 20 predictors from training \ntest &lt;- test %&gt;%\n  select(id, all_of(top_features))\n\n# set up cvfolds with v = 10\nset.seed(100)\nfolds &lt;- vfold_cv(train,v=10)\n\n\n## model1 \n\n# Update recipe to include only the top features\nvote_recipe &lt;- recipe(winner ~ . , data = train) %&gt;%\n  step_impute_mean(all_numeric_predictors(), -all_outcomes()) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes())\n\n#create the new model with ranger and classification \nrf_model1 &lt;- rand_forest() %&gt;%\n  set_engine('ranger')%&gt;%\n  set_mode('classification')\n\n\n# Update the workflow with the refined recipe\nrf_wkfl1 &lt;- workflow() %&gt;%\n  add_model(rf_model1)%&gt;%\n  add_recipe(vote_recipe)\n\n#Convert \n\n#update random forest model with mtry, min_m and trees \nrf_model1 &lt;- rand_forest(\n  mtry = tune(),     \n  min_n = tune(),    \n  trees = 1000      \n) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\n#update workflow with tuning model\nrf_tune_wkfl1 &lt;- rf_wkfl1 %&gt;%\n  update_model(rf_model1)\n\n#set up tuning parameters \nrf_params1 &lt;- parameters(\n  mtry(range = c(1, ncol(train) - 1)),\n  min_n(range = c(1, 10))\n)\n\n# Set up Bayesian Optimization\nbayes_opt1 &lt;- tune_bayes(\n  rf_model1,\n  vote_recipe,\n  resamples = vfold_cv(train, v = 5),\n  param_info = rf_params1,\n  initial = 10,  \n  iter = 20,     \n  metrics = metric_set(roc_auc, accuracy)\n)\n#check the model results \nbayes_opt1 %&gt;%\n  collect_metrics()\n\n#select the best model based on their roc_auc results \nbest_params1 &lt;- select_best(bayes_opt1, metric = \"roc_auc\")\n\n#create final workflow with the best model\nfinal_rf_wkfl1 &lt;- rf_tune_wkfl1 %&gt;%\n  finalize_workflow(best_params1)\nfinal_rf_wkfl1\n\n#resample the model with the newest workflow and folds\nset.seed(100)\nfinal_rf_res1 &lt;- fit_resamples(\n  final_rf_wkfl1,\n  resamples =folds,\n  metrics = metric_set(accuracy,roc_auc),control=control_resamples(save_workflow = TRUE)\n)\n\ncollect_metrics(final_rf_res1)\n\n# fit the training data to the workflow \nfinal_rf_fit1 &lt;-final_rf_wkfl1 %&gt;%\n  fit(data=train)\n\n#make predictions based on the model \nrf_predictions1 &lt;- final_rf_fit1 %&gt;%\n  predict(new_data=test)\n\n#create results table \nresults_rf1 &lt;- test %&gt;%\n  select(id) %&gt;%\n  bind_cols(rf_predictions1)%&gt;%\n  rename(id = id, winner = .pred_class)\n\nhead(results_rf1,15)\n\n#output results \nwrite_csv(results_rf1,'rf_bayes_class_original_top20_final.csv')\n\nrm(list = ls())\n\n\n## model2 stacking\n\n# load all required packages \nlibrary(ROSE)\nlibrary(ISLR)\nlibrary(caret)\nlibrary(kernlab)\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(tune)\nlibrary(dials)\nlibrary(ranger)\nlibrary(workflows)\nlibrary(rsample)\nlibrary(yardstick)\nlibrary(kknn)\nlibrary(lattice)\nlibrary(stacks)\ntidymodels_prefer()\n\n# load the original training data \ntrain &lt;- read_csv('train_class.csv',show_col_types = FALSE)\n\n# delete the id, name and total_population to remove non-predictors and ovoid overfitting.  \ntrain &lt;- train %&gt;% \n  select(-id, -name, -x0001e)\n\n#create recipe from training data \nvote_recipe1 &lt;- recipe(winner ~ . , data = train) %&gt;%\n  step_impute_mean(all_numeric_predictors(), -all_outcomes()) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes())\n\n# set engine and mode for rain forest \nrf_initial_model &lt;- rand_forest(trees = 100) %&gt;%\n  set_engine('ranger', importance = 'impurity') %&gt;%\n  set_mode('classification')\n\n# Workflow to fit initial model\ninitial_rf_wkfl &lt;- workflow() %&gt;%\n  add_model(rf_initial_model) %&gt;%\n  add_recipe(vote_recipe1)\n\n# Fit model to get feature importance\nset.seed(100)\ninitial_fit &lt;- fit(initial_rf_wkfl, data = train)\n\n# Extracting feature importance\nimportance_df &lt;- vip::vi(initial_fit, method = \"model\")\n# Check the structure of importance_df\nprint(str(importance_df))\n\n#select the top 20 important predictors\ntop_features &lt;- importance_df$Variable[1:20]\n\n# select only the response variable and top 20 predictors \ntrain &lt;- train %&gt;%\n  select(winner, all_of(top_features))\n\n#load the test data \ntest &lt;- read_csv('test_class.csv')\n\n# select only the id column and the top 20 predictors from training \ntest &lt;- test %&gt;%\n  select(id, all_of(top_features))\n\n# set up cvfolds with v = 10\nset.seed(100)\nfolds &lt;- vfold_cv(train,v=10, repeats = 3)\n\n# Set control for tuning\nctrl_grid &lt;- control_stack_grid()\n\n# Set up resampling\nset.seed(100)\ncv_folds &lt;- vfold_cv(train, v = 10, repeats = 3)\n\n# Update recipe to include only the top features\nvote_recipe &lt;- recipe(winner ~ . , data = train) %&gt;%\n  step_impute_mean(all_numeric_predictors(), -all_outcomes()) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes())\n\n# create random forest model \nrf_model &lt;- rand_forest() %&gt;%\n  set_engine('ranger')%&gt;%\n  set_mode('classification')\n\n\n# Update the workflow with the refined recipe\nrf_wkfl &lt;- workflow() %&gt;%\n  add_model(rf_model)%&gt;%\n  add_recipe(vote_recipe)\n\n#update model with tuning parameters mtry, min_n, trees  \nrf_tune_model &lt;- rand_forest(\n  mtry = tune(),     # Indicate tuning\n  min_n = tune(),    # Indicate tuning\n  trees = 1000       # Fixed number of trees\n) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\n#update workflow with tuning model \nrf_tune_wkfl &lt;- rf_wkfl %&gt;%\n  update_model(rf_tune_model)\n\n#set up tuning parameters \nrf_params &lt;- parameters(\n  mtry(range = c(1, ncol(train) - 1)),\n  min_n(range = c(1, 10))\n)\n\n# Set up Bayesian Optimization\nbayes_opt &lt;- tune_bayes(\n  rf_tune_model,\n  vote_recipe,\n  resamples = vfold_cv(train, v = 5),\n  param_info = rf_params,\n  initial = 10,  # Number of randomly selected points to evaluate before the Bayesian optimization starts\n  iter = 20,     # Number of iterations of Bayesian optimization\n  metrics = metric_set(roc_auc, accuracy)\n)\n\nbayes_opt %&gt;%\n  collect_metrics()\n\n#select the best model based on roc_auc \nbest_params &lt;- select_best(bayes_opt, metric = \"roc_auc\")\n\n#update workflow with the selected model \nfinal_rf_wkfl &lt;- rf_tune_wkfl %&gt;%\n  finalize_workflow(best_params)\nfinal_rf_wkfl\n\n# resample the final workflow with folds \nset.seed(100)\nfolds &lt;- vfold_cv(train,v=10)\n\nfinal_rf_res &lt;- fit_resamples(\n  final_rf_wkfl,\n  resamples =folds,\n  metrics = metric_set(accuracy,roc_auc),control=control_resamples(save_workflow = TRUE)\n)\n\n## random forest 2\n\n#set up second random forest model \nrf_model2 &lt;- rand_forest() %&gt;%\n  set_engine('ranger')%&gt;%\n  set_mode('classification')\n\n# Update the workflow with the refined recipe\nrf_wkfl2 &lt;- workflow() %&gt;%\n  add_model(rf_model2)%&gt;%\n  add_recipe(vote_recipe)\n\n#update the second model with tuning parameters mtry, trees, min_n\nrf_tune_model2 &lt;- rand_forest(\n  mtry = tune(),\n  trees = tune(),\n  min_n = tune()\n) %&gt;%\n  set_engine('ranger')%&gt;%\n  set_mode('classification')\n\n#update the workflow with tuning model \nrf_tune_wkfl2 &lt;- rf_wkfl2 %&gt;%\n  update_model(rf_tune_model2)\n\n#set up tuning parameters \nrf_grid2 &lt;- grid_latin_hypercube(\n  mtry(range = c(1, 5)),\n  trees(range = c(50, 200)),\n  min_n(range = c(1, 10)),\n  size = 20\n)\n\n\n#apply grid tuning \nrf_tuning2 &lt;- tune_grid(\n  rf_tune_wkfl2,\n  resamples = folds,\n  grid = rf_grid2,\n  metrics = metric_set(accuracy, roc_auc)\n)\n\nrf_tuning2 %&gt;%\n  collect_metrics()\n\n#select the best model with roc_auc results \nbest_rf_model2 &lt;- rf_tuning2 %&gt;%\n  select_best(metric = 'roc_auc')\n\n#update the final workflow with the best model \nfinal_rf_wkfl2 &lt;- rf_tune_wkfl2 %&gt;%\n  finalize_workflow(best_rf_model2)\n\n#resample the data with the final workflow and folds \nset.seed(100)\nfinal_rf_res2 &lt;- fit_resamples(\n  final_rf_wkfl2,\n  resamples =folds,\n  metrics = metric_set(accuracy,roc_auc),control=control_resamples(save_workflow = TRUE)\n)\n\n\n## Model 3 SVM-rbf with bayes optimization\n\n# Define the SVM model with tuning parameters\nsvm_rbf_model &lt;- svm_rbf() %&gt;%\n  set_engine(\"kernlab\") %&gt;%\n  set_mode('classification') %&gt;%\n  set_args(cost = tune(), rbf_sigma = tune())\n\n# Define the recipe\nvote_recipe_svm &lt;- recipe(winner ~ ., data = train) %&gt;%\n  step_impute_mean(all_numeric(), -all_outcomes()) %&gt;%\n  step_normalize(all_numeric()) %&gt;%\n  step_nzv(all_predictors())\n\n# Define the workflow\nsvm_rbf_wkfl &lt;- workflow() %&gt;%\n  add_model(svm_rbf_model) %&gt;%\n  add_recipe(vote_recipe_svm)\n\n# Define the parameter set\nsvm_param &lt;- extract_parameter_set_dials(svm_rbf_wkfl) %&gt;%\n  update(\n    rbf_sigma = rbf_sigma(range = c(-7, -1)),\n    cost = cost(range = c(-5, 2))\n  )\n\n# Define the initial grid using Latin hypercube\nsvm_grid &lt;- grid_latin_hypercube(\n  svm_param,\n  size = 30\n)\n\n# Set seed for reproducibility\nset.seed(100)\n\n# Initial tuning with grid search\nsvm_initial &lt;- tune_grid(\n  svm_rbf_wkfl,\n  resamples = folds,\n  grid = svm_grid,\n  metrics = metric_set(accuracy, roc_auc)\n)\n\n# Collect initial metrics\ncollect_metrics(svm_initial)\n\n# Bayesian optimization control settings\nctrl &lt;- control_bayes(verbose = FALSE)\n\n# Bayesian optimization\nset.seed(100)\nsvm_bo &lt;- tune_bayes(\n  svm_rbf_wkfl,\n  resamples = folds,\n  metrics = metric_set(accuracy, roc_auc),\n  initial = svm_initial,\n  param_info = svm_param,\n  iter = 25,\n  control = ctrl\n)\n\n# Display best results\nsvm_bo %&gt;% show_best(metric = 'roc_auc', n = 5)\n\n# Select the best model from Bayesian optimization\nbest_bayes &lt;- svm_bo %&gt;% select_best(metric = \"roc_auc\")\n\n# Finalize the workflow with the best parameters\nfinal_svm_rbf_wkfl &lt;- finalize_workflow(svm_rbf_wkfl, best_bayes)\n\n# Perform resampling with the final workflow\nset.seed(100)\n\nfinal_svm_rbf_res &lt;- fit_resamples(\n  final_svm_rbf_wkfl,\n  resamples = folds,\n  metrics = metric_set(accuracy, roc_auc),\n  control=control_stack_resamples()\n)\n\n# Collect final resampling metrics\ncollect_metrics(final_svm_rbf_res)\n\n## logistic model \n\n# Define the logistic regression model with glmnet, suitable for tuning\nlogistic_tuned &lt;- logistic_reg(penalty = tune(), mixture = tune()) %&gt;% \n  set_engine(\"glmnet\") %&gt;% \n  set_mode(\"classification\")\n\n\n# Combine the recipe and model into a workflow\nworkflow_tuned &lt;- workflow() %&gt;%\n  add_model(logistic_tuned) %&gt;%\n  add_recipe(vote_recipe)\n\n# Define a grid of hyperparameters\npenalty_vals &lt;- penalty(range = c(-6, -1), trans = log10_trans()) # Log transformation\nmixture_vals &lt;- mixture()\n\n# Create a regular grid\ntuning_grid &lt;- grid_regular(\n  penalty_vals,\n  mixture_vals,\n  levels = 10\n)\n\ntune_results &lt;- tune_grid(\n  workflow_tuned,\n  resamples = folds,\n  grid = tuning_grid,\n  metrics = metric_set(roc_auc, accuracy)\n)\n\ntune_results %&gt;% collect_metrics()\n\n# Select the best model based on ROC AUC\nbest_model3 &lt;- select_best(tune_results, metric = \"roc_auc\")\n\n# Finalize the workflow with the best model\nfinal_workflow &lt;- finalize_workflow(workflow_tuned, best_model3)\n\n# Fit the finalized workflow to the resampled training data\nset.seed(100)  # Ensure reproducibility\nfinal_results &lt;- fit_resamples(\n  final_workflow,\n  resamples = cv_folds,\n  metrics = metric_set(roc_auc, accuracy), \n  control=control_resamples(save_workflow = TRUE)\n)\n\n\n# stacking \n\n#set up for model stacking \nstack_control &lt;- control_resamples(\n  save_pred = TRUE,   \n  save_workflow = TRUE  \n)\n\nset.seed(100)\n# Random Forest Model with bayes tuning resample \nrf_res1 &lt;- fit_resamples(\n  final_rf_wkfl,\n  resamples =folds,\n  metrics = metric_set(roc_auc),\n  control= stack_control\n)\n#random forest model with grid tuning resample \nrf_res2 &lt;- fit_resamples(\n  final_rf_wkfl2,\n  resamples =folds,\n  metrics = metric_set(roc_auc),\n  control=stack_control\n)\n#svm with bayes tuning resample \nsvm_res &lt;- fit_resamples(\n  final_svm_rbf_wkfl,\n  resamples = folds,\n  metrics = metric_set(roc_auc), \n  control=stack_control\n)\n#logistics model resample \nlog_res &lt;- fit_resamples(\n  final_workflow,\n  resamples = folds,\n  metrics = metric_set(roc_auc), \n  control=stack_control\n)\n\n\n# Stack the models\nstack &lt;- stacks() %&gt;%\n  add_candidates(rf_res1) %&gt;%\n  add_candidates(rf_res2) %&gt;%\n  add_candidates(log_res) %&gt;%\n  add_candidates(svm_res)\n\n# Blend predictions\nstack &lt;- stack %&gt;%\n  blend_predictions() %&gt;%\n  fit_members()\n\n#check the model by predicting the training data \ntrain_predictions &lt;- predict(stack, new_data = train)\nresults_df &lt;- train %&gt;%\n  bind_cols(train_predictions)\n\nresults_df$winner &lt;- as.factor(results_df$winner)\nresults_df$.pred_class &lt;- as.factor(results_df$.pred_class)\n\n# Calculate accuracy\naccuracy_result &lt;- accuracy(data = results_df, truth = \"winner\", estimate = \".pred_class\")\n\n# Print accuracy\nprint(accuracy_result)\n\n# predict the test dataset \nnew_prediction &lt;- predict(stack, new_data = test)\n\nresults_stack &lt;- test %&gt;%\n  select(id) %&gt;% \n  bind_cols(new_prediction)\n\nhead(results_stack)"
  },
  {
    "objectID": "project/project1.html",
    "href": "project/project1.html",
    "title": "Data Analysis of Life Expectancy in 2007",
    "section": "",
    "text": "As we are experiencing a better lifestyle now, we do care more about how well we can maintain that quality of life and how long we can live to enjoy our lives. For this project, we want to study how the status of developed/developing countries, BMI, alcohol consumption, general government expenditure on health, thinness among children and adolescents for Age 10 to 19, and Hepatitis B coverage affected the life expectancy in 2007 to understand different factors contributing to health problems globally.\nThe dataset originates from the Global Health Observatory (GHO) data repository, under the auspices of the World Health Organization (WHO), a research project conducted from 2000 to 2015 across 193 countries. The choice to focus on data from the year 2007 was strategic, as it contains the fewest missing values (NAs) and may offer a more comprehensive and reliable basis for analysis within the specified timeframe.\nThis report looks closely at the basic setup of the raw data, providing a detailed analysis of both full models and transformed models. It evaluates the goodness of fit across various variance selection methods to identify the most suitable final model to explain our dataset. Finally, we’ll discuss how we can use our findings in real life situations and their limitations.\nThis is my team’s report for the Analysis"
  },
  {
    "objectID": "project/project3.html",
    "href": "project/project3.html",
    "title": "Amazon Order Survey",
    "section": "",
    "text": "This report explains the process of constructing regression models for a train dataset through supervised learning. The dataset used in this study is retrieved from Kaggle, for which the data come from a survey conducted on approximately 5000 randomly selected Amazon customers from January 2018 to December 2022. The dataset contains eight files: train.csv, test.csv, customer_info_test, customer_info_train, amazon_order_details_test, amazon_order_details_train, data_descriptions.txt, and sample_submission.csv. For the purpose of this specific study, we use the train.csv and test.csv and modify them accordingly in the process of building the final regression model.\nThe objective of this study is to build a model such that it will predict the variable log_total, that addresses the skewness in the variable order_totals (calculated by totaling the cost of items in the amazon_order_details files) by taking log-base 10, based on various predictors. According to the data_description.txt, we select the predictor columns that include distinct age levels, income levels, and how many people are using the same account levels with statistical significance to our prediction\nThis is my team’s report for the Regression Model"
  },
  {
    "objectID": "project/code/classification.html",
    "href": "project/code/classification.html",
    "title": "Code Display",
    "section": "",
    "text": "# load all required packages \nlibrary(ROSE)\nlibrary(ISLR)\nlibrary(caret)\nlibrary(kernlab)\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(tune)\nlibrary(dials)\nlibrary(ranger)\nlibrary(workflows)\nlibrary(rsample)\nlibrary(yardstick)\nlibrary(kknn)\nlibrary(lattice)\nlibrary(stacks)\ntidymodels_prefer()\n\n# load the original training data \ntrain &lt;- read_csv('train_class.csv',show_col_types = FALSE)\n\n# delete the id, name and total_population to remove non-predictors and ovoid overfitting.  \ntrain &lt;- train %&gt;% \n  select(-id, -name, -x0001e)\n\n#create recipe from training data \nvote_recipe1 &lt;- recipe(winner ~ . , data = train) %&gt;%\n  step_impute_mean(all_numeric_predictors(), -all_outcomes()) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes())\n\n# set engine and mode for rain forest \nrf_initial_model &lt;- rand_forest(trees = 100) %&gt;%\n  set_engine('ranger', importance = 'impurity') %&gt;%\n  set_mode('classification')\n\n# Workflow to fit initial model\ninitial_rf_wkfl &lt;- workflow() %&gt;%\n  add_model(rf_initial_model) %&gt;%\n  add_recipe(vote_recipe1)\n\n# Fit model to get feature importance\nset.seed(100)\ninitial_fit &lt;- fit(initial_rf_wkfl, data = train)\n\n# Extracting feature importance\nimportance_df &lt;- vip::vi(initial_fit, method = \"model\")\n# Check the structure of importance_df\nprint(str(importance_df))\n\n#select the top 20 important predictors\ntop_features &lt;- importance_df$Variable[1:20]\n\n# select only the response variable and top 20 predictors \ntrain &lt;- train %&gt;%\n  select(winner, all_of(top_features))\n\n#load the test data \ntest &lt;- read_csv('test_class.csv')\n\n# select only the id column and the top 20 predictors from training \ntest &lt;- test %&gt;%\n  select(id, all_of(top_features))\n\n# set up cvfolds with v = 10\nset.seed(100)\nfolds &lt;- vfold_cv(train,v=10)\n\n\n## model1 \n\n# Update recipe to include only the top features\nvote_recipe &lt;- recipe(winner ~ . , data = train) %&gt;%\n  step_impute_mean(all_numeric_predictors(), -all_outcomes()) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes())\n\n#create the new model with ranger and classification \nrf_model1 &lt;- rand_forest() %&gt;%\n  set_engine('ranger')%&gt;%\n  set_mode('classification')\n\n\n# Update the workflow with the refined recipe\nrf_wkfl1 &lt;- workflow() %&gt;%\n  add_model(rf_model1)%&gt;%\n  add_recipe(vote_recipe)\n\n#Convert \n\n#update random forest model with mtry, min_m and trees \nrf_model1 &lt;- rand_forest(\n  mtry = tune(),     \n  min_n = tune(),    \n  trees = 1000      \n) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\n#update workflow with tuning model\nrf_tune_wkfl1 &lt;- rf_wkfl1 %&gt;%\n  update_model(rf_model1)\n\n#set up tuning parameters \nrf_params1 &lt;- parameters(\n  mtry(range = c(1, ncol(train) - 1)),\n  min_n(range = c(1, 10))\n)\n\n# Set up Bayesian Optimization\nbayes_opt1 &lt;- tune_bayes(\n  rf_model1,\n  vote_recipe,\n  resamples = vfold_cv(train, v = 5),\n  param_info = rf_params1,\n  initial = 10,  \n  iter = 20,     \n  metrics = metric_set(roc_auc, accuracy)\n)\n#check the model results \nbayes_opt1 %&gt;%\n  collect_metrics()\n\n#select the best model based on their roc_auc results \nbest_params1 &lt;- select_best(bayes_opt1, metric = \"roc_auc\")\n\n#create final workflow with the best model\nfinal_rf_wkfl1 &lt;- rf_tune_wkfl1 %&gt;%\n  finalize_workflow(best_params1)\nfinal_rf_wkfl1\n\n#resample the model with the newest workflow and folds\nset.seed(100)\nfinal_rf_res1 &lt;- fit_resamples(\n  final_rf_wkfl1,\n  resamples =folds,\n  metrics = metric_set(accuracy,roc_auc),control=control_resamples(save_workflow = TRUE)\n)\n\ncollect_metrics(final_rf_res1)\n\n# fit the training data to the workflow \nfinal_rf_fit1 &lt;-final_rf_wkfl1 %&gt;%\n  fit(data=train)\n\n#make predictions based on the model \nrf_predictions1 &lt;- final_rf_fit1 %&gt;%\n  predict(new_data=test)\n\n#create results table \nresults_rf1 &lt;- test %&gt;%\n  select(id) %&gt;%\n  bind_cols(rf_predictions1)%&gt;%\n  rename(id = id, winner = .pred_class)\n\nhead(results_rf1,15)\n\n#output results \nwrite_csv(results_rf1,'rf_bayes_class_original_top20_final.csv')\n\nrm(list = ls())\n\n\n## model2 stacking\n\n# load all required packages \nlibrary(ROSE)\nlibrary(ISLR)\nlibrary(caret)\nlibrary(kernlab)\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(tune)\nlibrary(dials)\nlibrary(ranger)\nlibrary(workflows)\nlibrary(rsample)\nlibrary(yardstick)\nlibrary(kknn)\nlibrary(lattice)\nlibrary(stacks)\ntidymodels_prefer()\n\n# load the original training data \ntrain &lt;- read_csv('train_class.csv',show_col_types = FALSE)\n\n# delete the id, name and total_population to remove non-predictors and ovoid overfitting.  \ntrain &lt;- train %&gt;% \n  select(-id, -name, -x0001e)\n\n#create recipe from training data \nvote_recipe1 &lt;- recipe(winner ~ . , data = train) %&gt;%\n  step_impute_mean(all_numeric_predictors(), -all_outcomes()) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes())\n\n# set engine and mode for rain forest \nrf_initial_model &lt;- rand_forest(trees = 100) %&gt;%\n  set_engine('ranger', importance = 'impurity') %&gt;%\n  set_mode('classification')\n\n# Workflow to fit initial model\ninitial_rf_wkfl &lt;- workflow() %&gt;%\n  add_model(rf_initial_model) %&gt;%\n  add_recipe(vote_recipe1)\n\n# Fit model to get feature importance\nset.seed(100)\ninitial_fit &lt;- fit(initial_rf_wkfl, data = train)\n\n# Extracting feature importance\nimportance_df &lt;- vip::vi(initial_fit, method = \"model\")\n# Check the structure of importance_df\nprint(str(importance_df))\n\n#select the top 20 important predictors\ntop_features &lt;- importance_df$Variable[1:20]\n\n# select only the response variable and top 20 predictors \ntrain &lt;- train %&gt;%\n  select(winner, all_of(top_features))\n\n#load the test data \ntest &lt;- read_csv('test_class.csv')\n\n# select only the id column and the top 20 predictors from training \ntest &lt;- test %&gt;%\n  select(id, all_of(top_features))\n\n# set up cvfolds with v = 10\nset.seed(100)\nfolds &lt;- vfold_cv(train,v=10, repeats = 3)\n\n# Set control for tuning\nctrl_grid &lt;- control_stack_grid()\n\n# Set up resampling\nset.seed(100)\ncv_folds &lt;- vfold_cv(train, v = 10, repeats = 3)\n\n# Update recipe to include only the top features\nvote_recipe &lt;- recipe(winner ~ . , data = train) %&gt;%\n  step_impute_mean(all_numeric_predictors(), -all_outcomes()) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes())\n\n# create random forest model \nrf_model &lt;- rand_forest() %&gt;%\n  set_engine('ranger')%&gt;%\n  set_mode('classification')\n\n\n# Update the workflow with the refined recipe\nrf_wkfl &lt;- workflow() %&gt;%\n  add_model(rf_model)%&gt;%\n  add_recipe(vote_recipe)\n\n#update model with tuning parameters mtry, min_n, trees  \nrf_tune_model &lt;- rand_forest(\n  mtry = tune(),     # Indicate tuning\n  min_n = tune(),    # Indicate tuning\n  trees = 1000       # Fixed number of trees\n) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\n#update workflow with tuning model \nrf_tune_wkfl &lt;- rf_wkfl %&gt;%\n  update_model(rf_tune_model)\n\n#set up tuning parameters \nrf_params &lt;- parameters(\n  mtry(range = c(1, ncol(train) - 1)),\n  min_n(range = c(1, 10))\n)\n\n# Set up Bayesian Optimization\nbayes_opt &lt;- tune_bayes(\n  rf_tune_model,\n  vote_recipe,\n  resamples = vfold_cv(train, v = 5),\n  param_info = rf_params,\n  initial = 10,  # Number of randomly selected points to evaluate before the Bayesian optimization starts\n  iter = 20,     # Number of iterations of Bayesian optimization\n  metrics = metric_set(roc_auc, accuracy)\n)\n\nbayes_opt %&gt;%\n  collect_metrics()\n\n#select the best model based on roc_auc \nbest_params &lt;- select_best(bayes_opt, metric = \"roc_auc\")\n\n#update workflow with the selected model \nfinal_rf_wkfl &lt;- rf_tune_wkfl %&gt;%\n  finalize_workflow(best_params)\nfinal_rf_wkfl\n\n# resample the final workflow with folds \nset.seed(100)\nfolds &lt;- vfold_cv(train,v=10)\n\nfinal_rf_res &lt;- fit_resamples(\n  final_rf_wkfl,\n  resamples =folds,\n  metrics = metric_set(accuracy,roc_auc),control=control_resamples(save_workflow = TRUE)\n)\n\n## random forest 2\n\n#set up second random forest model \nrf_model2 &lt;- rand_forest() %&gt;%\n  set_engine('ranger')%&gt;%\n  set_mode('classification')\n\n# Update the workflow with the refined recipe\nrf_wkfl2 &lt;- workflow() %&gt;%\n  add_model(rf_model2)%&gt;%\n  add_recipe(vote_recipe)\n\n#update the second model with tuning parameters mtry, trees, min_n\nrf_tune_model2 &lt;- rand_forest(\n  mtry = tune(),\n  trees = tune(),\n  min_n = tune()\n) %&gt;%\n  set_engine('ranger')%&gt;%\n  set_mode('classification')\n\n#update the workflow with tuning model \nrf_tune_wkfl2 &lt;- rf_wkfl2 %&gt;%\n  update_model(rf_tune_model2)\n\n#set up tuning parameters \nrf_grid2 &lt;- grid_latin_hypercube(\n  mtry(range = c(1, 5)),\n  trees(range = c(50, 200)),\n  min_n(range = c(1, 10)),\n  size = 20\n)\n\n\n#apply grid tuning \nrf_tuning2 &lt;- tune_grid(\n  rf_tune_wkfl2,\n  resamples = folds,\n  grid = rf_grid2,\n  metrics = metric_set(accuracy, roc_auc)\n)\n\nrf_tuning2 %&gt;%\n  collect_metrics()\n\n#select the best model with roc_auc results \nbest_rf_model2 &lt;- rf_tuning2 %&gt;%\n  select_best(metric = 'roc_auc')\n\n#update the final workflow with the best model \nfinal_rf_wkfl2 &lt;- rf_tune_wkfl2 %&gt;%\n  finalize_workflow(best_rf_model2)\n\n#resample the data with the final workflow and folds \nset.seed(100)\nfinal_rf_res2 &lt;- fit_resamples(\n  final_rf_wkfl2,\n  resamples =folds,\n  metrics = metric_set(accuracy,roc_auc),control=control_resamples(save_workflow = TRUE)\n)\n\n\n## Model 3 SVM-rbf with bayes optimization\n\n# Define the SVM model with tuning parameters\nsvm_rbf_model &lt;- svm_rbf() %&gt;%\n  set_engine(\"kernlab\") %&gt;%\n  set_mode('classification') %&gt;%\n  set_args(cost = tune(), rbf_sigma = tune())\n\n# Define the recipe\nvote_recipe_svm &lt;- recipe(winner ~ ., data = train) %&gt;%\n  step_impute_mean(all_numeric(), -all_outcomes()) %&gt;%\n  step_normalize(all_numeric()) %&gt;%\n  step_nzv(all_predictors())\n\n# Define the workflow\nsvm_rbf_wkfl &lt;- workflow() %&gt;%\n  add_model(svm_rbf_model) %&gt;%\n  add_recipe(vote_recipe_svm)\n\n# Define the parameter set\nsvm_param &lt;- extract_parameter_set_dials(svm_rbf_wkfl) %&gt;%\n  update(\n    rbf_sigma = rbf_sigma(range = c(-7, -1)),\n    cost = cost(range = c(-5, 2))\n  )\n\n# Define the initial grid using Latin hypercube\nsvm_grid &lt;- grid_latin_hypercube(\n  svm_param,\n  size = 30\n)\n\n# Set seed for reproducibility\nset.seed(100)\n\n# Initial tuning with grid search\nsvm_initial &lt;- tune_grid(\n  svm_rbf_wkfl,\n  resamples = folds,\n  grid = svm_grid,\n  metrics = metric_set(accuracy, roc_auc)\n)\n\n# Collect initial metrics\ncollect_metrics(svm_initial)\n\n# Bayesian optimization control settings\nctrl &lt;- control_bayes(verbose = FALSE)\n\n# Bayesian optimization\nset.seed(100)\nsvm_bo &lt;- tune_bayes(\n  svm_rbf_wkfl,\n  resamples = folds,\n  metrics = metric_set(accuracy, roc_auc),\n  initial = svm_initial,\n  param_info = svm_param,\n  iter = 25,\n  control = ctrl\n)\n\n# Display best results\nsvm_bo %&gt;% show_best(metric = 'roc_auc', n = 5)\n\n# Select the best model from Bayesian optimization\nbest_bayes &lt;- svm_bo %&gt;% select_best(metric = \"roc_auc\")\n\n# Finalize the workflow with the best parameters\nfinal_svm_rbf_wkfl &lt;- finalize_workflow(svm_rbf_wkfl, best_bayes)\n\n# Perform resampling with the final workflow\nset.seed(100)\n\nfinal_svm_rbf_res &lt;- fit_resamples(\n  final_svm_rbf_wkfl,\n  resamples = folds,\n  metrics = metric_set(accuracy, roc_auc),\n  control=control_stack_resamples()\n)\n\n# Collect final resampling metrics\ncollect_metrics(final_svm_rbf_res)\n\n## logistic model \n\n# Define the logistic regression model with glmnet, suitable for tuning\nlogistic_tuned &lt;- logistic_reg(penalty = tune(), mixture = tune()) %&gt;% \n  set_engine(\"glmnet\") %&gt;% \n  set_mode(\"classification\")\n\n\n# Combine the recipe and model into a workflow\nworkflow_tuned &lt;- workflow() %&gt;%\n  add_model(logistic_tuned) %&gt;%\n  add_recipe(vote_recipe)\n\n# Define a grid of hyperparameters\npenalty_vals &lt;- penalty(range = c(-6, -1), trans = log10_trans()) # Log transformation\nmixture_vals &lt;- mixture()\n\n# Create a regular grid\ntuning_grid &lt;- grid_regular(\n  penalty_vals,\n  mixture_vals,\n  levels = 10\n)\n\ntune_results &lt;- tune_grid(\n  workflow_tuned,\n  resamples = folds,\n  grid = tuning_grid,\n  metrics = metric_set(roc_auc, accuracy)\n)\n\ntune_results %&gt;% collect_metrics()\n\n# Select the best model based on ROC AUC\nbest_model3 &lt;- select_best(tune_results, metric = \"roc_auc\")\n\n# Finalize the workflow with the best model\nfinal_workflow &lt;- finalize_workflow(workflow_tuned, best_model3)\n\n# Fit the finalized workflow to the resampled training data\nset.seed(100)  # Ensure reproducibility\nfinal_results &lt;- fit_resamples(\n  final_workflow,\n  resamples = cv_folds,\n  metrics = metric_set(roc_auc, accuracy), \n  control=control_resamples(save_workflow = TRUE)\n)\n\n\n# stacking \n\n#set up for model stacking \nstack_control &lt;- control_resamples(\n  save_pred = TRUE,   \n  save_workflow = TRUE  \n)\n\nset.seed(100)\n# Random Forest Model with bayes tuning resample \nrf_res1 &lt;- fit_resamples(\n  final_rf_wkfl,\n  resamples =folds,\n  metrics = metric_set(roc_auc),\n  control= stack_control\n)\n#random forest model with grid tuning resample \nrf_res2 &lt;- fit_resamples(\n  final_rf_wkfl2,\n  resamples =folds,\n  metrics = metric_set(roc_auc),\n  control=stack_control\n)\n#svm with bayes tuning resample \nsvm_res &lt;- fit_resamples(\n  final_svm_rbf_wkfl,\n  resamples = folds,\n  metrics = metric_set(roc_auc), \n  control=stack_control\n)\n#logistics model resample \nlog_res &lt;- fit_resamples(\n  final_workflow,\n  resamples = folds,\n  metrics = metric_set(roc_auc), \n  control=stack_control\n)\n\n\n# Stack the models\nstack &lt;- stacks() %&gt;%\n  add_candidates(rf_res1) %&gt;%\n  add_candidates(rf_res2) %&gt;%\n  add_candidates(log_res) %&gt;%\n  add_candidates(svm_res)\n\n# Blend predictions\nstack &lt;- stack %&gt;%\n  blend_predictions() %&gt;%\n  fit_members()\n\n#check the model by predicting the training data \ntrain_predictions &lt;- predict(stack, new_data = train)\nresults_df &lt;- train %&gt;%\n  bind_cols(train_predictions)\n\nresults_df$winner &lt;- as.factor(results_df$winner)\nresults_df$.pred_class &lt;- as.factor(results_df$.pred_class)\n\n# Calculate accuracy\naccuracy_result &lt;- accuracy(data = results_df, truth = \"winner\", estimate = \".pred_class\")\n\n# Print accuracy\nprint(accuracy_result)\n\n# predict the test dataset \nnew_prediction &lt;- predict(stack, new_data = test)\n\nresults_stack &lt;- test %&gt;%\n  select(id) %&gt;% \n  bind_cols(new_prediction)\n\nhead(results_stack)"
  }
]